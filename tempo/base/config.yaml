# use curl http://localhost:3200/status/config after connecting to any of components to get current config
http_api_prefix: ""
server:
  http_listen_port: 3200
  # 20 mb- above max_bytes_per_trace, as querying the trace data from frontend
  # can return values bigger than that.
  grpc_server_max_recv_msg_size: 20_000_000
  grpc_server_max_send_msg_size: 50_000_000

memberlist:
  abort_if_cluster_join_fails: false
  bind_port: 7946
  join_members:
    - tempo-gossip-ring:7946

metrics_generator:
  storage:
    path: /var/tempo/wal
    remote_write:
      - url: http://prometheus-0.prometheus-push:9090/api/v1/write
        send_exemplars: true
      - url: http://prometheus-1.prometheus-push:9090/api/v1/write
        send_exemplars: true
  processor:
    service_graphs:
      wait: 30s
      max_items: 50_000
      workers: 20
      enable_client_server_prefix: true
      dimensions:
        - k8s.cluster.name
        - k8s.namespace.name
        - net.host.name
        - service.namespace
        - uw.team
    span_metrics:
      dimensions:
        - k8s.namespace.name
        - uw.team

query_frontend:
  max_batch_size: 8
  search:
    default_result_limit: 20
    max_result_limit: 500
    concurrent_jobs: 10_000
    max_duration: 0

querier:
  max_concurrent_queries: 500
  trace_by_id:
    query_timeout: 60s

  frontend_worker:
    frontend_address: tempo-query-frontend-discovery:9095
    grpc_client_config:
      max_recv_msg_size: 20_000_000
      max_send_msg_size: 50_000_000

distributor:
  receivers:
    kafka:
      protocol_version: 2.6.0
      brokers:
        - kafka-shared-0.kafka-shared-headless.pubsub.svc.cluster.aws:9092
        - kafka-shared-1.kafka-shared-headless.pubsub.svc.cluster.aws:9092
        - kafka-shared-2.kafka-shared-headless.pubsub.svc.cluster.aws:9092
      group_id: processor-tempo
      topic: "otel.otlp_spans"
      client_id: processor-tempo
      auth:
        tls:
          ca_file: /kafka-client-certificate/ca.crt
          cert_file: /kafka-client-certificate/tls.crt
          key_file: /kafka-client-certificate/tls.key
          reload_interval: 1h

ingester:
  max_block_duration: 5m
  # 100 MB for a block in ingester
  max_block_bytes: 100_000_000
  lifecycler:
    ring:
      replication_factor: 3

storage:
  trace:
    wal:
      path: /var/tempo/wal

    block:
      version: vParquet3
      # https://grafana.com/docs/tempo/latest/operations/dedicated_columns/
      #
      # Dedicated attribute columns are limited to 10 span attributes and 10 resource attributes with string values.
      # As a rule of thumb, good candidates for dedicated attribute columns are attributes that contribute the most 
      # to the block size, even if they are not frequently queried. Reducing the generic attribute key-value list 
      # size significantly improves query performance.
      parquet_dedicated_columns:
        # resource
        - name: deployment.environment
          type: string
          scope: resource
        - name: host.name
          type: string
          scope: resource
        - name: net.host.name
          type: string
          scope: resource
        - name: service.version
          type: string
          scope: resource
        - name: service.namespace
          type: string
          scope: resource
        - name: uw.team
          type: string
          scope: resource
        # span
        - name: cerbos.policy.name
          type: string
          scope: span
        - name: db.statement
          type: string
          scope: span
        - name: db.system
          type: string
          scope: span
        - name: issuer
          type: string
          scope: span
        - name: messaging.source.name
          type: string
          scope: span
        - name: messaging.operation
          type: string
          scope: span
        - name: net.peer.name
          type: string
          scope: span
        - name: rpc.service
          type: string
          scope: span
        - name: rpc.method
          type: string
          scope: span

    pool:
      max_workers: 1000
      queue_depth: 20_000

    backend: s3
    s3:
      bucket: ${S3_BUCKET}
      endpoint: s3.eu-west-1.amazonaws.com
      hedge_requests_at: 500ms

    cache: memcached
    memcached:
      consistent_hash: true
      host: tempo-memcached
      service: memcached-client
      timeout: 1000ms

    search:
      cache_control:
        footer: true

compactor:
  compaction:
    # 30 days
    block_retention: 720h
    # 500MB max block bytes
    max_block_bytes: 500_000_000
    compaction_window: 1h
    compaction_cycle: 30s
  ring:
    kvstore:
      store: memberlist

overrides:
  defaults:
    global:
      # 5mb
      # Spans added to blocks over that limit will be dropped
      # thus, it doesn't really guarantee there will be no bigger traces,
      # but it limits them enough to allow querying any trace.
      max_bytes_per_trace: 5_000_000
    ingestion:
      max_traces_per_user: 0
    metrics_generator:
      processors:
        - service-graphs
        - span-metrics
      forwarder:
        queue_size: 10_000
        workers: 20
      processor:
        service_graphs:
          enable_client_server_prefix: true
